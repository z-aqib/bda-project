version: "3.8"

services:
  airflow-db:
    image: postgres:14
    container_name: airflow-db
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow
    networks:
      - bda-network
    volumes:
      - airflow_db_data:/var/lib/postgresql/data

  airflow:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: airflow
    depends_on:
      - airflow-db
    networks:
      - bda-network
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@airflow-db:5432/airflow
      AIRFLOW__CORE__LOAD_EXAMPLES: "false"
      AIRFLOW__WEBSERVER__EXPOSE_CONFIG: "true"
      AIRFLOW__WEBSERVER__WEB_SERVER_HOST: "0.0.0.0"
      AIRFLOW__WEBSERVER__WEB_SERVER_PORT: "8080"
      AIRFLOW__LOGGING__LOG_RETENTION_DAYS: "3"
      AIRFLOW__LOGGING__CLEANUP_FREQUENCY: "30"

      # ===== Your pipeline config (edit only if needed) =====
      MONGO_URI: mongodb://mongo:27017/iot_database
      MONGO_DB: iot_database
      MONGO_COLLECTION: sensor_readings

      ARCHIVE_THRESHOLD_MB: "300"
      ARCHIVE_KEEP_MINUTES: "15"          # keep last 15 minutes in Mongo, archive older
      ARCHIVE_CHUNK_DOCS: "20000"
      ARCHIVE_MIN_FREE_MB: "800"
      ARCHIVE_MAX_CHUNKS: "10"
      LOCAL_ARCHIVE_DIR: /opt/airflow/archive

      # HDFS container name (change if your namenode container has a different name)
      HDFS_CONTAINER: namenode
      HDFS_ARCHIVE_DIR: /bda/archive/mongo
      HDFS_METADATA_DIR: /bda/archive/metadata

      # Spark container name + path to your app inside that container
      SPARK_CONTAINER: spark
      SPARK_APP_PATH: /app/spark_analytics_dashboard.py

    volumes:
      - ./dags:/opt/airflow/dags
      - ./logs:/opt/airflow/logs
      - ./plugins:/opt/airflow/plugins
      - ./archive:/opt/airflow/archive
      - /var/run/docker.sock:/var/run/docker.sock
    ports:
      - "8080:8080"
    command: >
      bash -lc "
      airflow db migrate &&
      airflow users create
        --username admin
        --firstname Admin
        --lastname User
        --role Admin
        --email admin@airflow.local
        --password admin || true &&
      airflow scheduler &
      exec airflow webserver --host 0.0.0.0 --port 8080
      "
    restart: unless-stopped

volumes:
  airflow_db_data:

networks:
  bda-network:
    external: true
